{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "from afinn import Afinn\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 : Two functions: one to clean string columns (name, formatted_address, and latest_reviews) by retaining only English characters and spaces, and another to clean numerical columns (rating and user_ratings_total) by removing non-numeric characters and converting them to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_csv = 'Places Dataset.xlsx - places_final_dataset.csv'\n",
    "df_places= pd.read_csv(places_csv)\n",
    "\n",
    "df_places.info(), df_places.head()\n",
    "\n",
    "\n",
    "def clean_encoding(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace(\"Ã¢Â€Â™\", \"'\")\n",
    "        text = text.replace(\"Ã¢Â€Â\", '\"')\n",
    "        \n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        \n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def safe_literal_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "df_places['latest_reviews'] = df_places['latest_reviews'].apply(clean_encoding)\n",
    "\n",
    "df_places['latest_reviews'] = df_places['latest_reviews'].apply(safe_literal_eval)\n",
    "\n",
    "print(df_places['latest_reviews'].head(2))\n",
    "\n",
    "df_cleaned = pd.read_csv('cleaned_places_data.csv')\n",
    "\n",
    "def clean(name):\n",
    "    name = re.sub(r'[^\\x00-\\x7F]+', '', name)\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "df_cleaned['name'] = df_cleaned['name'].apply(clean)\n",
    "df_cleaned['formatted_address'] = df_cleaned['formatted_address'].apply(clean)\n",
    "\n",
    "df_cleaned.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02: The provided script preprocesses a dataset containing place reviews and ratings by first cleaning the data to remove rows with missing values in the 'latest_reviews' or 'rating' columns. It then performs sentiment analysis on the reviews using the SentimentIntensityAnalyzer from NLTK, extracting sentiment features such as negativity, neutrality, positivity, and compound scores. Additionally, it applies TF-IDF vectorization to convert text reviews into numerical features. These sentiment and TF-IDF features are combined and used to train a RandomForestRegressor model to predict ratings. The model's performance is evaluated with Mean Squared Error and R² score. For rows with missing ratings, sentiment analysis and TF-IDF vectorization are applied to predict the missing values, which are then filled in the dataset. The updated dataset, with predictions filled in, is saved to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "df = pd.read_csv('finalcleaned_places_dataset.csv')  \n",
    "\n",
    "df_clean = df.dropna(subset=['latest_reviews', 'rating'])\n",
    "\n",
    "df_clean['rating'] = df_clean['rating'].astype(float)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def extract_sentiment(review):\n",
    "    sentiment = sia.polarity_scores(str(review))\n",
    "    return pd.Series([sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound']])\n",
    "\n",
    "sentiment_features = df_clean['latest_reviews'].apply(extract_sentiment)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_features = tfidf.fit_transform(df_clean['latest_reviews'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "X = pd.concat([sentiment_features.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "y = df_clean['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Model Mean Squared Error: {mse:.2f}')\n",
    "print(f'Model R² Score: {r2:.2f}')\n",
    "\n",
    "df_missing = df[df['rating'].isnull()]\n",
    "\n",
    "sentiment_missing = df_missing['latest_reviews'].apply(extract_sentiment)\n",
    "\n",
    "tfidf_missing = tfidf.transform(df_missing['latest_reviews'])\n",
    "tfidf_missing_df = pd.DataFrame(tfidf_missing.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "X_missing = pd.concat([sentiment_missing.reset_index(drop=True), tfidf_missing_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "predicted_ratings = model.predict(X_missing)\n",
    "\n",
    "df.loc[df['rating'].isnull(), 'rating'] = predicted_ratings\n",
    "\n",
    "df.to_csv('2Rating_updated_dataset_with_sentiment_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predicted ratings (with sentiment analysis) have been filled and saved to 'Rating_updated_dataset_with_sentiment_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 : The provided script loads a dataset with reviews and ratings from a CSV file and preprocesses the text data in the 'latest_reviews' column. It starts by downloading the NLTK stopwords and defining a set of common English stopwords. The clean_reviews function removes these stopwords from each review by tokenizing the text, filtering out stopwords, and then rejoining the cleaned tokens into a string. This function is applied to the 'latest_reviews' column of the dataset. The cleaned reviews are displayed and optionally saved to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '2Rating_updated_dataset_with_sentiment_predictions.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_reviews(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = text.split()\n",
    "        cleaned_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "        return ' '.join(cleaned_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['latest_reviews'] = df['latest_reviews'].apply(clean_reviews)\n",
    "\n",
    "df[['name', 'latest_reviews']].head()\n",
    "\n",
    "cleaned_file_path = '2finalcleaned_places_reviews.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 : The provided script performs text processing on the 'latest_reviews' column of a dataset by retaining only nouns and verbs. It begins by downloading necessary NLTK data for tokenization and part-of-speech tagging. The filter_nouns_verbs_unique function tokenizes the text, applies part-of-speech tagging, filters out only the nouns and verbs, and then removes duplicate words by converting the list to a set and back to a list. This function is applied to the 'latest_reviews' column, replacing the original review texts with lists of unique nouns and verbs. The processed data is then saved to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "file_path = '2finalcleaned_places_reviews.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def filter_nouns_verbs_unique(text):\n",
    "    tokens = word_tokenize(text)  \n",
    "    tagged = pos_tag(tokens)  \n",
    "    filtered_words = [word for word, pos in tagged if pos.startswith('NN') or pos.startswith('VB')]  \n",
    "    unique_words = list(set(filtered_words))  \n",
    "    return unique_words \n",
    "\n",
    "df['latest_reviews'] = df['latest_reviews'].apply(lambda x: filter_nouns_verbs_unique(str(x)))\n",
    "\n",
    "output_path = '3filtered_reviews_override.csv'  \n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Filtered reviews saved to\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 : This Python script processes a dataset of location-based reviews using the pandas library. It reads data from a CSV file, defines a function to merge and deduplicate review lists, and then aggregates the data by location name. For each location, it retains the first occurrence of latitude, longitude, formatted address, and user ratings total, calculates the mean rating, and applies the review merging function to ensure unique reviews. The cleaned and aggregated dataset is then saved to a new CSV file, with a confirmation message printed to indicate the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '3filtered_reviews_override.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def merge_reviews(reviews):\n",
    "    unique_reviews = set()\n",
    "    for review_list in reviews:\n",
    "        review_items = eval(review_list)\n",
    "        unique_reviews.update(review_items)  \n",
    "    return list(unique_reviews)\n",
    "\n",
    "merged_df = df.groupby('name').agg({\n",
    "    'lat': 'first',  \n",
    "    'lng': 'first',  \n",
    "    'formatted_address': 'first',  \n",
    "    'rating': 'mean',  \n",
    "    'user_ratings_total': 'first',  \n",
    "    'latest_reviews': merge_reviews  \n",
    "}).reset_index()\n",
    "\n",
    "output_file_path = '4merged_unique_reviews.csv'  \n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with Groq API and Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is designed to categorize Sri Lankan tourist destinations based on their latest reviews.The code leverages the LLM's capabilities to automatically categorize Sri Lankan tourist destinations based on their reviews, providing valuable insights for travelers and tourism planners by using Groq API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_name_places_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(df, column_name):\n",
    "  \n",
    "    df[column_name] = df[column_name].astype(str).str.replace(r'[\\[\\]\"\\']', '', regex=True).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_column(df, 'latest_reviews')\n",
    "clean_text_column(df, 'name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('visitor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activities = []\n",
    "for activities_str in df['Preferred Activities']:\n",
    "    activities_list = ast.literal_eval(activities_str)  \n",
    "    all_activities.extend(activities_list)  \n",
    "\n",
    "unique_individual_activities = list(all_activities)\n",
    "unique_individual_activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('4merged_unique_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful assistant that gives the best matching activity category(s) from these for the input Sri Lankan tourist Destinations and their latest reviews\n",
    "            Activity Categories = ['cycling', 'historical monuments', 'village homestays', 'butterfly watching', 'hot springs', 'wildlife viewing', 'sea cruises', 'themed parks', 'craft workshops', 'fishing', 'sailing', 'history tours', 'literary tours', 'public art installations', 'temple pilgrimages', 'architecture tours', 'golfing', 'hot air ballooning', 'spiritual retreats', 'cultural experiences', 'botanical gardens', 'boat safaris', 'caving', 'cultural festivals', 'museum visits', 'mountain biking', 'camping', 'turtle watching', 'historic walks', 'safaris', 'waterfalls', 'scuba diving', 'elephant rides', 'bird watching', 'ayurvedic spa treatments', 'horse shows', 'traditional ceremonies', 'surfing', 'historic sites', 'art classes', 'city tours', 'theater', 'amusement parks', 'architecture photography', 'beachfront dining', 'kayaking', 'beach visits', 'rock climbing', 'arts and culture', 'snorkeling', 'animal encounters', 'archaeological sites', 'sailing lessons', 'whale watching', 'local crafts', 'yoga retreats', 'paddleboarding', 'horseback riding', 'zip-lining', 'outdoor adventures', 'planetarium visits', 'water parks', 'photography', 'sightseeing', 'tea tasting', 'hiking', 'river cruises', 'landscape photography']\n",
    "\n",
    "            ONLY OUTPUT THE CATEGORY(S) ONLY DON'T ADD EXPLANATIONS\n",
    "        ''',\n",
    "        ),\n",
    "        (\"human\", \"Destination: {destination}\\nLatest Reviews: {reviews}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "def process_destination_with_reviews(row):\n",
    "    result = chain.invoke({\n",
    "        \"destination\": row['name'],\n",
    "        \"reviews\": row['latest_reviews']\n",
    "    })\n",
    "    print(result.content)\n",
    "    return result.content\n",
    "\n",
    "df['categoriess'] = df.apply(process_destination_with_reviews, axis=1)\n",
    "\n",
    "print(df[['name', 'latest_reviews', 'categoriess']].head())\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "df.to_csv('updated_destinations_with_categories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('places_v7.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('places_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('places_v7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "\n",
    "    for word in words:\n",
    "        score = afinn.score(word)\n",
    "        if score > 0:\n",
    "            positive_words.append(word)\n",
    "        elif score < 0:\n",
    "            negative_words.append(word)\n",
    "        else:\n",
    "            neutral_words.append(word)\n",
    "\n",
    "    return len(positive_words), len(negative_words), len(neutral_words)\n",
    "\n",
    "df[['positive_words', 'negative_words', 'neutral_words']] = df1['latest_reviews'].apply(process_sentence).apply(pd.Series)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['neutral_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories']  = df['categoriess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_ratings_total'] = df['user_ratings_total'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('places_v7.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('visitor.csv')\n",
    "\n",
    "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "df['Bucket list destinations Sri Lanka'] = df['Bucket list destinations Sri Lanka'].apply(ast.literal_eval)\n",
    "\n",
    "all_destinations = df['Bucket list destinations Sri Lanka'].explode().unique()\n",
    "unique_destination_count = len(all_destinations)\n",
    "\n",
    "\n",
    "print(f\"The number of unique destinations in 'Bucket list destinations Sri Lanka' column is: {unique_destination_count}\")\n",
    "\n",
    "print(all_destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_destinations = ['Polonnaruwa' 'Hatton' 'Anuradhapura' 'Ella' 'Haputale'\n",
    " 'Madunagala Hot Water Spring' 'Wilpattu National Park'\n",
    " 'Wasgamuwa National Park' 'Kanneliya National Rain Forest Reserve'\n",
    " 'Horton Plains National Park' 'Mirissa Beach' 'Negombo Lagoon'\n",
    " 'Batadombalena Craft Centre' 'Jungle Beach' 'Bentota'\n",
    " 'Maha Oya Hot Water Springs' 'Colombo Port City' 'Trincomalee Harbour'\n",
    " 'Kalpitiya' 'Galle Dutch Fort' 'Sigiriya' 'Jaffna Public Library'\n",
    " 'Colombo' 'Mihintale' 'Dambulla Royal Cave Temple and Golden Temple'\n",
    " 'Hikkaduwa' 'Nuwara Eliya Golf Club' 'Kandalama' \"Sri Pada / Adam's Peak\"\n",
    " 'Seetha Eliya' 'Sri Dalada Maligawa'\n",
    " 'Seethawaka Wet Zone Botanical Gardens' 'Kandy Temple'\n",
    " 'Arankelle Forest Monastery' 'Batatotalena (Batadombalena) Cave'\n",
    " 'Madu River' 'Ritigala' 'Kandy National Museum' 'Folk Museum'\n",
    " 'Victoria Golf Club' 'Colombo National Museum' 'Meemure' 'Horton Plains'\n",
    " 'Udawalawe National Park' 'Ratnapura Gem Museum' 'Rekawa Beach'\n",
    " 'Kandy Lake' 'Galle Fort' 'Anuradapura' 'Polonaruwa' 'Diyaluma Falls'\n",
    " 'Dunhinda Waterfall' 'Yala National Park' 'Trincomalee'\n",
    " \"St Clair's Falls\" 'Udawalawe' 'Sinharaja Forest Reserve'\n",
    " 'Kumana National Park' 'Bundala National Park' 'Nallur Kandaswamy Kovil'\n",
    " 'Kandy' 'Nuwara Eliya' 'Tangalle' 'Weligama Beach (surf and stay)'\n",
    " 'Ramboda Falls' \"Baker's Falls\" 'Bomburu Ella Waterfall'\n",
    " 'Polonnaruwa Ancient City' 'Galle' 'Pinnawala' 'Colombo City Tour'\n",
    " 'Kandy City Centre' 'Nelung Arts Centre' 'Excel World'\n",
    " 'Dry Zone Botanic Gardens, Hambantota' 'Unawatuna' 'Knuckles'\n",
    " 'Kalpitiya Lagoon' 'Bentota River' 'Kitulgala' 'Passikuda Beach'\n",
    " 'Bentota Beach' 'Marakolliya Beach' 'Bopath Falls' 'Pigeon Island'\n",
    " 'Hikkaduwa Beach' 'Unawatuna Beach' 'Elephant Transit Home'\n",
    " 'Leisure World' 'Ruhunu Maha Kataragama Dewalaya' 'Negombo'\n",
    " 'Tangalle Beach' 'Gangaramaya Temple' 'Minneriya National Park'\n",
    " 'Kitugala Forest' 'Nilaveli Beach' 'National Gallery of Art'\n",
    " 'Maritime Museum' 'National Museum Galle' 'Dutch Museum'\n",
    " 'Mahapelessa Hot Springs' 'Arugam Bay Beach' 'Yapahuwa Rock Fortress'\n",
    " 'Royal Botanical Gardens, Peradeniya' 'Negambo' 'Royal Colombo Golf Club'\n",
    " 'Sri Lanka Planetarium' 'Dambulla' 'Hakgala Botanical Garden'\n",
    " 'Unawatuna Lagoon' 'Mahalenama Cave' 'Kithulgala' 'Water World Lanka'\n",
    " 'Pearl Bay' 'Nine Arches Bridge' 'Pinnawala Elephant Orphanage'\n",
    " 'Museum of Modern and Contemporary Art' 'Surathali Ella'\n",
    " 'Nelum Pokuna Theatre' 'Weligama Beach' 'Belilena Caves'\n",
    " 'Galle City Tour' 'Bolgoda Lake' 'Ambalangoda Mask Workshop' 'Belihuloya'\n",
    " 'Perl Bay' 'Ahungalla' 'Nallur Kandaswamy Devasthanam'\n",
    " 'Velgam Vehera Buddhist Temple' 'Ambuluwawa Tower'\n",
    " 'Anawilundawa Wetlands' 'Ella Rock' 'Ambalangoda' 'Riverstone Gap'\n",
    " 'Hikkaduwa Coral Sanctuary' 'Bambarakiri Ella' 'Jaya Sri Maha Bodhi'\n",
    " 'Ahangama' 'Viharamahadevi Park' 'Pidurangala Rock' 'Galle Lighthouse'\n",
    " 'Martin Wickramasinghe Folk Museum' ' Laxapana Falls'\n",
    " 'Ravan Ella Waterfall' 'Wavulpone Cave' 'Lionel Wendt Art Centre'\n",
    " 'Koggala Beach' 'Kosgoda Turtle Hatchery' 'Devon Falls'\n",
    " 'Bambarakanda Falls' 'Hiriketiya' 'Vaddha Village Camping'\n",
    " 'Kanniya Hot Springs' 'Jungle beach' 'Ella Gap' 'Hiriketiya Beach'\n",
    " 'Uppuveli Beach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('places_v7.csv')\n",
    "\n",
    "famous_destinations = ['Polonnaruwa', 'Hatton', 'Anuradhapura', 'Ella', 'Haputale', 'Madunagala Hot Water Spring', 'Wilpattu National Park', 'Wasgamuwa National Park', 'Kanneliya National Rain Forest Reserve', 'Horton Plains National Park', 'Mirissa Beach', 'Negombo Lagoon', 'Batadombalena Craft Centre', 'Jungle Beach', 'Bentota', 'Maha Oya Hot Water Springs', 'Colombo Port City', 'Trincomalee Harbour', 'Kalpitiya', 'Galle Dutch Fort', 'Sigiriya', 'Jaffna Public Library', 'Colombo', 'Mihintale', 'Dambulla Royal Cave Temple and Golden Temple', 'Hikkaduwa', 'Nuwara Eliya Golf Club', 'Kandalama', \"Sri Pada / Adam's Peak\", 'Seetha Eliya', 'Sri Dalada Maligawa', 'Seethawaka Wet Zone Botanical Gardens', 'Kandy Temple', 'Arankelle Forest Monastery', 'Batatotalena (Batadombalena) Cave', 'Madu River', 'Ritigala', 'Kandy National Museum', 'Folk Museum', 'Victoria Golf Club', 'Colombo National Museum', 'Meemure', 'Horton Plains', 'Udawalawe National Park', 'Ratnapura Gem Museum', 'Rekawa Beach', 'Kandy Lake', 'Galle Fort', 'Anuradapura', 'Polonnaruwa', 'Diyaluma Falls', 'Dunhinda Waterfall', 'Yala National Park', 'Trincomalee', \"St Clair's Falls\", 'Udawalawe', 'Sinharaja Forest Reserve', 'Kumana National Park', 'Bundala National Park', 'Nallur Kandaswamy Kovil', 'Kandy', 'Nuwara Eliya', 'Tangalle', 'Weligama Beach (surf and stay)', 'Ramboda Falls', \"Baker's Falls\", 'Bomburu Ella Waterfall', 'Polonnaruwa Ancient City', 'Galle', 'Pinnawala', 'Colombo City Tour', 'Kandy City Centre', 'Nelung Arts Centre', 'Excel World', 'Dry Zone Botanic Gardens, Hambantota', 'Unawatuna', 'Knuckles', 'Kalpitiya Lagoon', 'Bentota River', 'Kitulgala', 'Passikuda Beach', 'Bentota Beach', 'Marakolliya Beach', 'Bopath Falls', 'Pigeon Island', 'Hikkaduwa Beach', 'Unawatuna Beach', 'Elephant Transit Home', 'Leisure World', 'Ruhunu Maha Kataragama Dewalaya', 'Negombo', 'Tangalle Beach', 'Gangaramaya Temple', 'Minneriya National Park', 'Kitugala Forest', 'Nilaveli Beach', 'National Gallery of Art', 'Maritime Museum', 'National Museum Galle', 'Dutch Museum', 'Mahapelessa Hot Springs', 'Arugam Bay Beach', 'Yapahuwa Rock Fortress', 'Royal Botanical Gardens, Peradeniya', 'Negambo', 'Royal Colombo Golf Club', 'Sri Lanka Planetarium', 'Dambulla', 'Hakgala Botanical Garden', 'Unawatuna Lagoon', 'Mahalenama Cave', 'Kithulgala', 'Water World Lanka', 'Pearl Bay', 'Nine Arches Bridge', 'Pinnawala Elephant Orphanage', 'Museum of Modern and Contemporary Art', 'Surathali Ella', 'Nelum Pokuna Theatre', 'Weligama Beach', 'Belilena Caves', 'Galle City Tour', 'Bolgoda Lake', 'Ambalangoda Mask Workshop', 'Belihuloya', 'Perl Bay', 'Ahungalla', 'Nallur Kandaswamy Devasthanam', 'Velgam Vehera Buddhist Temple', 'Ambuluwawa Tower', 'Anawilundawa Wetlands', 'Ella Rock', 'Ambalangoda', 'Riverstone Gap', 'Hikkaduwa Coral Sanctuary', 'Bambarakiri Ella', 'Jaya Sri Maha Bodhi', 'Ahangama', 'Viharamahadevi Park', 'Pidurangala Rock', 'Galle Lighthouse', 'Martin Wickramasinghe Folk Museum', ' Laxapana Falls', 'Ravan Ella Waterfall', 'Wavulpone Cave', 'Lionel Wendt Art Centre', 'Koggala Beach', 'Kosgoda Turtle Hatchery', 'Devon Falls', 'Bambarakanda Falls', 'Hiriketiya', 'Vaddha Village Camping', 'Kanniya Hot Springs', 'Jungle beach', 'Ella Gap', 'Hiriketiya Beach', 'Uppuveli Beach']\n",
    "\n",
    "df = df[df['name'].isin(famous_destinations)]\n",
    "\n",
    "print(f\"Shape of the filtered dataframe is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('places_v8.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techTrivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
